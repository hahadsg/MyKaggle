{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from mydatools.features_analyze import get_top_k_corr\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_path = './data/input/train.csv'\n",
    "tst_path = './data/input/test.csv'\n",
    "id_col = 'id'\n",
    "label_col = 'target'\n",
    "\n",
    "submission_path = './data/output/submission.csv'\n",
    "output_id_col = id_col\n",
    "output_label_col = label_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds_type</th>\n",
       "      <th>id</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>16</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>17</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  ds_type  id  ps_calc_01  ps_calc_02  ps_calc_03  ps_calc_04  ps_calc_05  \\\n",
       "0   train   7         0.6         0.5         0.2           3           1   \n",
       "1   train   9         0.3         0.1         0.3           2           1   \n",
       "2   train  13         0.5         0.7         0.1           2           2   \n",
       "3   train  16         0.6         0.9         0.1           2           4   \n",
       "4   train  17         0.4         0.6         0.0           2           2   \n",
       "\n",
       "   ps_calc_06  ps_calc_07  ps_calc_08   ...    ps_ind_13_bin  ps_ind_14  \\\n",
       "0          10           1          10   ...                0          0   \n",
       "1           9           5           8   ...                0          0   \n",
       "2           9           1           8   ...                0          0   \n",
       "3           7           1           8   ...                0          0   \n",
       "4           6           3          10   ...                0          0   \n",
       "\n",
       "   ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ps_reg_01  \\\n",
       "0         11              0              1              0        0.7   \n",
       "1          3              0              0              1        0.8   \n",
       "2         12              1              0              0        0.0   \n",
       "3          8              1              0              0        0.9   \n",
       "4          9              1              0              0        0.7   \n",
       "\n",
       "   ps_reg_02  ps_reg_03  target  \n",
       "0        0.2   0.718070     0.0  \n",
       "1        0.4   0.766078     0.0  \n",
       "2        0.0  -1.000000     0.0  \n",
       "3        0.2   0.580948     0.0  \n",
       "4        0.6   0.840759     0.0  \n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_df = pd.read_csv(trn_path)\n",
    "trn_df['ds_type'] = 'train'\n",
    "\n",
    "tst_df = pd.read_csv(tst_path)\n",
    "tst_df['ds_type'] = 'test'\n",
    "\n",
    "full_df = pd.concat([trn_df, tst_df])\n",
    "\n",
    "del(trn_df)\n",
    "del(tst_df)\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset type\n",
    "is_train = full_df['ds_type'] == 'train'\n",
    "is_test = full_df['ds_type'] == 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "def add_features(features):\n",
    "    if not isinstance(features, list):\n",
    "        features = [features]\n",
    "    global feature_columns\n",
    "    feature_columns.extend([f for f in features if f not in feature_columns])\n",
    "    \n",
    "def remove_features(features):\n",
    "    if not isinstance(features, list):\n",
    "        features = [features]\n",
    "    global feature_columns\n",
    "    feature_columns = [f for f in feature_columns if f not in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numerical_features = full_df.dtypes[full_df.dtypes != 'object'].index.tolist()\n",
    "numerical_features = [c for c in numerical_features if c not in [id_col, label_col, 'ds_type']]\n",
    "add_features(numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 空值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df.replace(-1, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import missingno as msno\n",
    "# msno.matrix(full_df[feature_columns], labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 去掉空值过多的特征\n",
    "remove_features(['ps_car_03_cat', 'ps_car_05_cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 处理categorical类型特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_features = [c for c in feature_columns if c.endswith('cat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ps_car_01_cat     12\n",
       "ps_car_02_cat      2\n",
       "ps_car_04_cat     10\n",
       "ps_car_06_cat     18\n",
       "ps_car_07_cat      2\n",
       "ps_car_08_cat      2\n",
       "ps_car_09_cat      5\n",
       "ps_car_10_cat      3\n",
       "ps_car_11_cat    104\n",
       "ps_ind_02_cat      4\n",
       "ps_ind_04_cat      2\n",
       "ps_ind_05_cat      7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features_card_num = full_df[categorical_features].apply(lambda x: x.nunique())\n",
    "categorical_features_card_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high_card_features: ['ps_car_01_cat', 'ps_car_04_cat', 'ps_car_06_cat', 'ps_car_11_cat']\n",
      "low_card_features: ['ps_car_02_cat', 'ps_car_07_cat', 'ps_car_08_cat', 'ps_car_09_cat', 'ps_car_10_cat', 'ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat']\n"
     ]
    }
   ],
   "source": [
    "# 高基数分类特征\n",
    "# high_card_features = ['ps_car_11_cat']\n",
    "high_card_features = categorical_features_card_num[categorical_features_card_num >= 10].index.tolist()\n",
    "low_card_features = [f for f in categorical_features if f not in high_card_features]\n",
    "print('high_card_features:', high_card_features)\n",
    "print('low_card_features:', low_card_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 高基数分类特征处理\n",
    "\n",
    "  target encoding: http://www.saedsayad.com/encoding.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in high_card_features:\n",
    "    trn, tst = target_encode(full_df[is_train][f], \n",
    "                             full_df[is_test][f],\n",
    "                             full_df[is_train][label_col], \n",
    "                             min_samples_leaf=100,\n",
    "                             smoothing=10,\n",
    "                             noise_level=0.01)\n",
    "    new_f = f+'_encode'\n",
    "    full_df[new_f] = trn.append(tst, ignore_index=True)\n",
    "    add_features(new_f)\n",
    "    remove_features(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 低基数分类特征\n",
    "\n",
    "  one-hot处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_df = pd.get_dummies(full_df[low_card_features], columns=low_card_features)\n",
    "remove_features(low_card_features)\n",
    "add_features(dummy_df.columns.tolist())\n",
    "full_df[dummy_df.columns] = dummy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_df = full_df[is_train]\n",
    "tst_df = full_df[is_test]\n",
    "\n",
    "X = trn_df[feature_columns]\n",
    "y = trn_df[label_col]\n",
    "X_trn, X_val, y_trn, y_val = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "X_tst = tst_df[feature_columns]\n",
    "\n",
    "trn_lgb = lgb.Dataset(X_trn.values, y_trn, free_raw_data=False)\n",
    "val_lgb = lgb.Dataset(X_val.values, y_val, free_raw_data=False, reference=trn_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gini coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(y, pred):\n",
    "    \"\"\"\n",
    "    gini = 2*roc_auc - 1\n",
    "    \"\"\"\n",
    "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(y) + 1) / 2.\n",
    "    return gs / len(y)\n",
    "\n",
    "def gini_lgb(preds, dtrain):\n",
    "    y = list(dtrain.get_label())\n",
    "    score = gini(y, preds) / gini(y, y)\n",
    "    return 'gini', score, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.608775\ttraining's gini: 0.217486\tvalid_1's auc: 0.582454\tvalid_1's gini: 0.164599\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[2]\ttraining's auc: 0.628274\ttraining's gini: 0.256617\tvalid_1's auc: 0.601738\tvalid_1's gini: 0.203395\n",
      "[3]\ttraining's auc: 0.635112\ttraining's gini: 0.270248\tvalid_1's auc: 0.610373\tvalid_1's gini: 0.220717\n",
      "[4]\ttraining's auc: 0.636021\ttraining's gini: 0.272053\tvalid_1's auc: 0.612002\tvalid_1's gini: 0.223996\n",
      "[5]\ttraining's auc: 0.636691\ttraining's gini: 0.273389\tvalid_1's auc: 0.612735\tvalid_1's gini: 0.225462\n",
      "[6]\ttraining's auc: 0.637231\ttraining's gini: 0.274463\tvalid_1's auc: 0.612736\tvalid_1's gini: 0.22547\n",
      "[7]\ttraining's auc: 0.637215\ttraining's gini: 0.274432\tvalid_1's auc: 0.612209\tvalid_1's gini: 0.224417\n",
      "[8]\ttraining's auc: 0.637635\ttraining's gini: 0.275272\tvalid_1's auc: 0.613391\tvalid_1's gini: 0.226784\n",
      "[9]\ttraining's auc: 0.639169\ttraining's gini: 0.278338\tvalid_1's auc: 0.614705\tvalid_1's gini: 0.229411\n",
      "[10]\ttraining's auc: 0.640181\ttraining's gini: 0.280364\tvalid_1's auc: 0.615636\tvalid_1's gini: 0.231271\n",
      "[11]\ttraining's auc: 0.640709\ttraining's gini: 0.281419\tvalid_1's auc: 0.615747\tvalid_1's gini: 0.231495\n",
      "[12]\ttraining's auc: 0.640714\ttraining's gini: 0.281428\tvalid_1's auc: 0.616123\tvalid_1's gini: 0.232246\n",
      "[13]\ttraining's auc: 0.641256\ttraining's gini: 0.282513\tvalid_1's auc: 0.616295\tvalid_1's gini: 0.23259\n",
      "[14]\ttraining's auc: 0.641542\ttraining's gini: 0.283083\tvalid_1's auc: 0.616619\tvalid_1's gini: 0.233238\n",
      "[15]\ttraining's auc: 0.641282\ttraining's gini: 0.282565\tvalid_1's auc: 0.616365\tvalid_1's gini: 0.232729\n",
      "[16]\ttraining's auc: 0.641616\ttraining's gini: 0.283232\tvalid_1's auc: 0.616671\tvalid_1's gini: 0.233342\n",
      "[17]\ttraining's auc: 0.642066\ttraining's gini: 0.284132\tvalid_1's auc: 0.616956\tvalid_1's gini: 0.233912\n",
      "[18]\ttraining's auc: 0.642627\ttraining's gini: 0.285253\tvalid_1's auc: 0.61713\tvalid_1's gini: 0.23426\n",
      "[19]\ttraining's auc: 0.642807\ttraining's gini: 0.285615\tvalid_1's auc: 0.617159\tvalid_1's gini: 0.234318\n",
      "[20]\ttraining's auc: 0.643119\ttraining's gini: 0.286237\tvalid_1's auc: 0.61712\tvalid_1's gini: 0.23424\n",
      "[21]\ttraining's auc: 0.6434\ttraining's gini: 0.286799\tvalid_1's auc: 0.617231\tvalid_1's gini: 0.234462\n",
      "[22]\ttraining's auc: 0.643439\ttraining's gini: 0.286877\tvalid_1's auc: 0.616955\tvalid_1's gini: 0.23391\n",
      "[23]\ttraining's auc: 0.643861\ttraining's gini: 0.287723\tvalid_1's auc: 0.617391\tvalid_1's gini: 0.234782\n",
      "[24]\ttraining's auc: 0.644215\ttraining's gini: 0.288431\tvalid_1's auc: 0.617733\tvalid_1's gini: 0.235467\n",
      "[25]\ttraining's auc: 0.644338\ttraining's gini: 0.288676\tvalid_1's auc: 0.617574\tvalid_1's gini: 0.235148\n",
      "[26]\ttraining's auc: 0.644579\ttraining's gini: 0.289157\tvalid_1's auc: 0.61783\tvalid_1's gini: 0.23566\n",
      "[27]\ttraining's auc: 0.644604\ttraining's gini: 0.289208\tvalid_1's auc: 0.617859\tvalid_1's gini: 0.235719\n",
      "[28]\ttraining's auc: 0.644888\ttraining's gini: 0.289776\tvalid_1's auc: 0.618436\tvalid_1's gini: 0.236871\n",
      "[29]\ttraining's auc: 0.644899\ttraining's gini: 0.289798\tvalid_1's auc: 0.618371\tvalid_1's gini: 0.236742\n",
      "[30]\ttraining's auc: 0.645063\ttraining's gini: 0.290126\tvalid_1's auc: 0.61842\tvalid_1's gini: 0.236839\n",
      "[31]\ttraining's auc: 0.645251\ttraining's gini: 0.290502\tvalid_1's auc: 0.618593\tvalid_1's gini: 0.237185\n",
      "[32]\ttraining's auc: 0.645615\ttraining's gini: 0.29123\tvalid_1's auc: 0.61893\tvalid_1's gini: 0.237859\n",
      "[33]\ttraining's auc: 0.645728\ttraining's gini: 0.291456\tvalid_1's auc: 0.618974\tvalid_1's gini: 0.237947\n",
      "[34]\ttraining's auc: 0.645835\ttraining's gini: 0.29167\tvalid_1's auc: 0.618952\tvalid_1's gini: 0.237904\n",
      "[35]\ttraining's auc: 0.646293\ttraining's gini: 0.292587\tvalid_1's auc: 0.619432\tvalid_1's gini: 0.238863\n",
      "[36]\ttraining's auc: 0.64645\ttraining's gini: 0.292899\tvalid_1's auc: 0.619417\tvalid_1's gini: 0.238835\n",
      "[37]\ttraining's auc: 0.64658\ttraining's gini: 0.293161\tvalid_1's auc: 0.619557\tvalid_1's gini: 0.239114\n",
      "[38]\ttraining's auc: 0.646658\ttraining's gini: 0.293316\tvalid_1's auc: 0.619655\tvalid_1's gini: 0.239309\n",
      "[39]\ttraining's auc: 0.64683\ttraining's gini: 0.29366\tvalid_1's auc: 0.619868\tvalid_1's gini: 0.239736\n",
      "[40]\ttraining's auc: 0.646982\ttraining's gini: 0.293963\tvalid_1's auc: 0.619941\tvalid_1's gini: 0.239882\n",
      "[41]\ttraining's auc: 0.647028\ttraining's gini: 0.294055\tvalid_1's auc: 0.619915\tvalid_1's gini: 0.23983\n",
      "[42]\ttraining's auc: 0.647124\ttraining's gini: 0.294248\tvalid_1's auc: 0.619947\tvalid_1's gini: 0.239895\n",
      "[43]\ttraining's auc: 0.647231\ttraining's gini: 0.294462\tvalid_1's auc: 0.619964\tvalid_1's gini: 0.239928\n",
      "[44]\ttraining's auc: 0.64757\ttraining's gini: 0.295139\tvalid_1's auc: 0.620206\tvalid_1's gini: 0.240413\n",
      "[45]\ttraining's auc: 0.647761\ttraining's gini: 0.295522\tvalid_1's auc: 0.620206\tvalid_1's gini: 0.240413\n",
      "[46]\ttraining's auc: 0.64779\ttraining's gini: 0.29558\tvalid_1's auc: 0.62014\tvalid_1's gini: 0.24028\n",
      "[47]\ttraining's auc: 0.648042\ttraining's gini: 0.296083\tvalid_1's auc: 0.620303\tvalid_1's gini: 0.240606\n",
      "[48]\ttraining's auc: 0.648124\ttraining's gini: 0.296248\tvalid_1's auc: 0.620203\tvalid_1's gini: 0.240406\n",
      "[49]\ttraining's auc: 0.648451\ttraining's gini: 0.296902\tvalid_1's auc: 0.620407\tvalid_1's gini: 0.240814\n",
      "[50]\ttraining's auc: 0.648651\ttraining's gini: 0.297301\tvalid_1's auc: 0.620518\tvalid_1's gini: 0.241036\n",
      "[51]\ttraining's auc: 0.648842\ttraining's gini: 0.297684\tvalid_1's auc: 0.620567\tvalid_1's gini: 0.241133\n",
      "[52]\ttraining's auc: 0.648879\ttraining's gini: 0.297758\tvalid_1's auc: 0.620639\tvalid_1's gini: 0.241277\n",
      "[53]\ttraining's auc: 0.64892\ttraining's gini: 0.29784\tvalid_1's auc: 0.620648\tvalid_1's gini: 0.241296\n",
      "[54]\ttraining's auc: 0.649002\ttraining's gini: 0.298005\tvalid_1's auc: 0.620615\tvalid_1's gini: 0.24123\n",
      "[55]\ttraining's auc: 0.649215\ttraining's gini: 0.29843\tvalid_1's auc: 0.620698\tvalid_1's gini: 0.241395\n",
      "[56]\ttraining's auc: 0.649555\ttraining's gini: 0.29911\tvalid_1's auc: 0.620987\tvalid_1's gini: 0.241975\n",
      "[57]\ttraining's auc: 0.649794\ttraining's gini: 0.299589\tvalid_1's auc: 0.621159\tvalid_1's gini: 0.242318\n",
      "[58]\ttraining's auc: 0.64994\ttraining's gini: 0.299881\tvalid_1's auc: 0.621267\tvalid_1's gini: 0.242534\n",
      "[59]\ttraining's auc: 0.650131\ttraining's gini: 0.300261\tvalid_1's auc: 0.621347\tvalid_1's gini: 0.242694\n",
      "[60]\ttraining's auc: 0.65035\ttraining's gini: 0.300699\tvalid_1's auc: 0.621508\tvalid_1's gini: 0.243016\n",
      "[61]\ttraining's auc: 0.650462\ttraining's gini: 0.300924\tvalid_1's auc: 0.621427\tvalid_1's gini: 0.242854\n",
      "[62]\ttraining's auc: 0.650621\ttraining's gini: 0.301241\tvalid_1's auc: 0.621531\tvalid_1's gini: 0.243062\n",
      "[63]\ttraining's auc: 0.65079\ttraining's gini: 0.301581\tvalid_1's auc: 0.621602\tvalid_1's gini: 0.243204\n",
      "[64]\ttraining's auc: 0.650957\ttraining's gini: 0.301915\tvalid_1's auc: 0.621666\tvalid_1's gini: 0.243331\n",
      "[65]\ttraining's auc: 0.651081\ttraining's gini: 0.302163\tvalid_1's auc: 0.621659\tvalid_1's gini: 0.243318\n",
      "[66]\ttraining's auc: 0.651268\ttraining's gini: 0.302537\tvalid_1's auc: 0.621804\tvalid_1's gini: 0.243609\n",
      "[67]\ttraining's auc: 0.651396\ttraining's gini: 0.302792\tvalid_1's auc: 0.621853\tvalid_1's gini: 0.243707\n",
      "[68]\ttraining's auc: 0.6516\ttraining's gini: 0.3032\tvalid_1's auc: 0.6221\tvalid_1's gini: 0.2442\n",
      "[69]\ttraining's auc: 0.651857\ttraining's gini: 0.303713\tvalid_1's auc: 0.622167\tvalid_1's gini: 0.244334\n",
      "[70]\ttraining's auc: 0.651993\ttraining's gini: 0.303986\tvalid_1's auc: 0.622164\tvalid_1's gini: 0.244329\n",
      "[71]\ttraining's auc: 0.65223\ttraining's gini: 0.304459\tvalid_1's auc: 0.62233\tvalid_1's gini: 0.24466\n",
      "[72]\ttraining's auc: 0.652284\ttraining's gini: 0.304568\tvalid_1's auc: 0.622257\tvalid_1's gini: 0.244513\n",
      "[73]\ttraining's auc: 0.652448\ttraining's gini: 0.304897\tvalid_1's auc: 0.622262\tvalid_1's gini: 0.244523\n",
      "[74]\ttraining's auc: 0.652724\ttraining's gini: 0.305447\tvalid_1's auc: 0.622501\tvalid_1's gini: 0.245001\n",
      "[75]\ttraining's auc: 0.652884\ttraining's gini: 0.305768\tvalid_1's auc: 0.622513\tvalid_1's gini: 0.245026\n",
      "[76]\ttraining's auc: 0.653081\ttraining's gini: 0.306162\tvalid_1's auc: 0.622638\tvalid_1's gini: 0.245277\n",
      "[77]\ttraining's auc: 0.653244\ttraining's gini: 0.306488\tvalid_1's auc: 0.6227\tvalid_1's gini: 0.245399\n",
      "[78]\ttraining's auc: 0.653435\ttraining's gini: 0.30687\tvalid_1's auc: 0.622815\tvalid_1's gini: 0.24563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79]\ttraining's auc: 0.653668\ttraining's gini: 0.307336\tvalid_1's auc: 0.622864\tvalid_1's gini: 0.245728\n",
      "[80]\ttraining's auc: 0.653901\ttraining's gini: 0.307802\tvalid_1's auc: 0.623021\tvalid_1's gini: 0.246042\n",
      "[81]\ttraining's auc: 0.654014\ttraining's gini: 0.308029\tvalid_1's auc: 0.623029\tvalid_1's gini: 0.246057\n",
      "[82]\ttraining's auc: 0.654174\ttraining's gini: 0.308349\tvalid_1's auc: 0.6231\tvalid_1's gini: 0.246201\n",
      "[83]\ttraining's auc: 0.654317\ttraining's gini: 0.308633\tvalid_1's auc: 0.623197\tvalid_1's gini: 0.246395\n",
      "[84]\ttraining's auc: 0.654388\ttraining's gini: 0.308776\tvalid_1's auc: 0.623246\tvalid_1's gini: 0.246492\n",
      "[85]\ttraining's auc: 0.654485\ttraining's gini: 0.30897\tvalid_1's auc: 0.623228\tvalid_1's gini: 0.246456\n",
      "[86]\ttraining's auc: 0.654743\ttraining's gini: 0.309486\tvalid_1's auc: 0.623447\tvalid_1's gini: 0.246894\n",
      "[87]\ttraining's auc: 0.654967\ttraining's gini: 0.309934\tvalid_1's auc: 0.623533\tvalid_1's gini: 0.247067\n",
      "[88]\ttraining's auc: 0.65505\ttraining's gini: 0.3101\tvalid_1's auc: 0.62354\tvalid_1's gini: 0.247081\n",
      "[89]\ttraining's auc: 0.65526\ttraining's gini: 0.31052\tvalid_1's auc: 0.623672\tvalid_1's gini: 0.247344\n",
      "[90]\ttraining's auc: 0.655494\ttraining's gini: 0.310988\tvalid_1's auc: 0.623677\tvalid_1's gini: 0.247354\n",
      "[91]\ttraining's auc: 0.655609\ttraining's gini: 0.311219\tvalid_1's auc: 0.623788\tvalid_1's gini: 0.247576\n",
      "[92]\ttraining's auc: 0.655869\ttraining's gini: 0.311737\tvalid_1's auc: 0.623883\tvalid_1's gini: 0.247767\n",
      "[93]\ttraining's auc: 0.656014\ttraining's gini: 0.312028\tvalid_1's auc: 0.62381\tvalid_1's gini: 0.24762\n",
      "[94]\ttraining's auc: 0.656209\ttraining's gini: 0.312417\tvalid_1's auc: 0.623834\tvalid_1's gini: 0.247668\n",
      "[95]\ttraining's auc: 0.656319\ttraining's gini: 0.312637\tvalid_1's auc: 0.623865\tvalid_1's gini: 0.247731\n",
      "[96]\ttraining's auc: 0.65652\ttraining's gini: 0.31304\tvalid_1's auc: 0.624012\tvalid_1's gini: 0.248025\n",
      "[97]\ttraining's auc: 0.656687\ttraining's gini: 0.313375\tvalid_1's auc: 0.623982\tvalid_1's gini: 0.247963\n",
      "[98]\ttraining's auc: 0.656867\ttraining's gini: 0.313734\tvalid_1's auc: 0.624072\tvalid_1's gini: 0.248143\n",
      "[99]\ttraining's auc: 0.657162\ttraining's gini: 0.314325\tvalid_1's auc: 0.624309\tvalid_1's gini: 0.248617\n",
      "[100]\ttraining's auc: 0.657321\ttraining's gini: 0.314641\tvalid_1's auc: 0.624409\tvalid_1's gini: 0.248818\n",
      "[101]\ttraining's auc: 0.657508\ttraining's gini: 0.315016\tvalid_1's auc: 0.624491\tvalid_1's gini: 0.248981\n",
      "[102]\ttraining's auc: 0.657691\ttraining's gini: 0.315382\tvalid_1's auc: 0.624644\tvalid_1's gini: 0.249289\n",
      "[103]\ttraining's auc: 0.657833\ttraining's gini: 0.315667\tvalid_1's auc: 0.624548\tvalid_1's gini: 0.249096\n",
      "[104]\ttraining's auc: 0.65799\ttraining's gini: 0.315979\tvalid_1's auc: 0.624615\tvalid_1's gini: 0.249229\n",
      "[105]\ttraining's auc: 0.658127\ttraining's gini: 0.316254\tvalid_1's auc: 0.624601\tvalid_1's gini: 0.249203\n",
      "[106]\ttraining's auc: 0.65831\ttraining's gini: 0.31662\tvalid_1's auc: 0.624632\tvalid_1's gini: 0.249264\n",
      "[107]\ttraining's auc: 0.658461\ttraining's gini: 0.316923\tvalid_1's auc: 0.624746\tvalid_1's gini: 0.249492\n",
      "[108]\ttraining's auc: 0.658589\ttraining's gini: 0.317177\tvalid_1's auc: 0.624778\tvalid_1's gini: 0.249556\n",
      "[109]\ttraining's auc: 0.658785\ttraining's gini: 0.31757\tvalid_1's auc: 0.624916\tvalid_1's gini: 0.249832\n",
      "[110]\ttraining's auc: 0.658905\ttraining's gini: 0.317811\tvalid_1's auc: 0.624875\tvalid_1's gini: 0.24975\n",
      "[111]\ttraining's auc: 0.659087\ttraining's gini: 0.318173\tvalid_1's auc: 0.624926\tvalid_1's gini: 0.249851\n",
      "[112]\ttraining's auc: 0.659274\ttraining's gini: 0.318549\tvalid_1's auc: 0.624987\tvalid_1's gini: 0.249975\n",
      "[113]\ttraining's auc: 0.659435\ttraining's gini: 0.31887\tvalid_1's auc: 0.625023\tvalid_1's gini: 0.250047\n",
      "[114]\ttraining's auc: 0.659552\ttraining's gini: 0.319104\tvalid_1's auc: 0.624958\tvalid_1's gini: 0.249917\n",
      "[115]\ttraining's auc: 0.659716\ttraining's gini: 0.319432\tvalid_1's auc: 0.624909\tvalid_1's gini: 0.249818\n",
      "[116]\ttraining's auc: 0.659931\ttraining's gini: 0.319862\tvalid_1's auc: 0.624955\tvalid_1's gini: 0.249909\n",
      "[117]\ttraining's auc: 0.660076\ttraining's gini: 0.320152\tvalid_1's auc: 0.625044\tvalid_1's gini: 0.250088\n",
      "[118]\ttraining's auc: 0.660172\ttraining's gini: 0.320344\tvalid_1's auc: 0.625062\tvalid_1's gini: 0.250123\n",
      "[119]\ttraining's auc: 0.66033\ttraining's gini: 0.320661\tvalid_1's auc: 0.625081\tvalid_1's gini: 0.250161\n",
      "[120]\ttraining's auc: 0.660497\ttraining's gini: 0.320994\tvalid_1's auc: 0.625157\tvalid_1's gini: 0.250313\n",
      "[121]\ttraining's auc: 0.660656\ttraining's gini: 0.321313\tvalid_1's auc: 0.625144\tvalid_1's gini: 0.250288\n",
      "[122]\ttraining's auc: 0.660769\ttraining's gini: 0.321537\tvalid_1's auc: 0.625138\tvalid_1's gini: 0.250276\n",
      "[123]\ttraining's auc: 0.660941\ttraining's gini: 0.321883\tvalid_1's auc: 0.62519\tvalid_1's gini: 0.250379\n",
      "[124]\ttraining's auc: 0.661105\ttraining's gini: 0.322211\tvalid_1's auc: 0.625172\tvalid_1's gini: 0.250343\n",
      "[125]\ttraining's auc: 0.661288\ttraining's gini: 0.322576\tvalid_1's auc: 0.625288\tvalid_1's gini: 0.250575\n",
      "[126]\ttraining's auc: 0.661419\ttraining's gini: 0.322837\tvalid_1's auc: 0.62529\tvalid_1's gini: 0.250579\n",
      "[127]\ttraining's auc: 0.661569\ttraining's gini: 0.323139\tvalid_1's auc: 0.625305\tvalid_1's gini: 0.250611\n",
      "[128]\ttraining's auc: 0.66171\ttraining's gini: 0.323421\tvalid_1's auc: 0.625278\tvalid_1's gini: 0.250555\n",
      "[129]\ttraining's auc: 0.661866\ttraining's gini: 0.323732\tvalid_1's auc: 0.625295\tvalid_1's gini: 0.25059\n",
      "[130]\ttraining's auc: 0.662082\ttraining's gini: 0.324163\tvalid_1's auc: 0.625381\tvalid_1's gini: 0.250762\n",
      "[131]\ttraining's auc: 0.66219\ttraining's gini: 0.324379\tvalid_1's auc: 0.625377\tvalid_1's gini: 0.250754\n",
      "[132]\ttraining's auc: 0.662441\ttraining's gini: 0.324883\tvalid_1's auc: 0.625567\tvalid_1's gini: 0.251134\n",
      "[133]\ttraining's auc: 0.662555\ttraining's gini: 0.32511\tvalid_1's auc: 0.625553\tvalid_1's gini: 0.251107\n",
      "[134]\ttraining's auc: 0.662704\ttraining's gini: 0.325408\tvalid_1's auc: 0.625585\tvalid_1's gini: 0.251171\n",
      "[135]\ttraining's auc: 0.662859\ttraining's gini: 0.325717\tvalid_1's auc: 0.625644\tvalid_1's gini: 0.251288\n",
      "[136]\ttraining's auc: 0.663\ttraining's gini: 0.325999\tvalid_1's auc: 0.625653\tvalid_1's gini: 0.251306\n",
      "[137]\ttraining's auc: 0.663113\ttraining's gini: 0.326225\tvalid_1's auc: 0.625731\tvalid_1's gini: 0.251463\n",
      "[138]\ttraining's auc: 0.66322\ttraining's gini: 0.32644\tvalid_1's auc: 0.625716\tvalid_1's gini: 0.251431\n",
      "[139]\ttraining's auc: 0.663395\ttraining's gini: 0.326789\tvalid_1's auc: 0.625828\tvalid_1's gini: 0.251656\n",
      "[140]\ttraining's auc: 0.663514\ttraining's gini: 0.327028\tvalid_1's auc: 0.625775\tvalid_1's gini: 0.251551\n",
      "[141]\ttraining's auc: 0.663642\ttraining's gini: 0.327283\tvalid_1's auc: 0.625807\tvalid_1's gini: 0.251614\n",
      "[142]\ttraining's auc: 0.663855\ttraining's gini: 0.327711\tvalid_1's auc: 0.626012\tvalid_1's gini: 0.252023\n",
      "[143]\ttraining's auc: 0.664013\ttraining's gini: 0.328026\tvalid_1's auc: 0.626047\tvalid_1's gini: 0.252095\n",
      "[144]\ttraining's auc: 0.664186\ttraining's gini: 0.328372\tvalid_1's auc: 0.626152\tvalid_1's gini: 0.252303\n",
      "[145]\ttraining's auc: 0.664341\ttraining's gini: 0.328683\tvalid_1's auc: 0.626206\tvalid_1's gini: 0.252412\n",
      "[146]\ttraining's auc: 0.664457\ttraining's gini: 0.328913\tvalid_1's auc: 0.626238\tvalid_1's gini: 0.252477\n",
      "[147]\ttraining's auc: 0.664576\ttraining's gini: 0.329153\tvalid_1's auc: 0.626263\tvalid_1's gini: 0.252525\n",
      "[148]\ttraining's auc: 0.664744\ttraining's gini: 0.329488\tvalid_1's auc: 0.626308\tvalid_1's gini: 0.252616\n",
      "[149]\ttraining's auc: 0.664914\ttraining's gini: 0.329829\tvalid_1's auc: 0.626297\tvalid_1's gini: 0.252593\n",
      "[150]\ttraining's auc: 0.665071\ttraining's gini: 0.330142\tvalid_1's auc: 0.626326\tvalid_1's gini: 0.252652\n",
      "[151]\ttraining's auc: 0.665213\ttraining's gini: 0.330426\tvalid_1's auc: 0.62635\tvalid_1's gini: 0.2527\n",
      "[152]\ttraining's auc: 0.665356\ttraining's gini: 0.330712\tvalid_1's auc: 0.62636\tvalid_1's gini: 0.25272\n",
      "[153]\ttraining's auc: 0.665524\ttraining's gini: 0.331048\tvalid_1's auc: 0.62636\tvalid_1's gini: 0.252721\n",
      "[154]\ttraining's auc: 0.665706\ttraining's gini: 0.331412\tvalid_1's auc: 0.626469\tvalid_1's gini: 0.252938\n",
      "[155]\ttraining's auc: 0.665835\ttraining's gini: 0.331669\tvalid_1's auc: 0.62641\tvalid_1's gini: 0.25282\n",
      "[156]\ttraining's auc: 0.666006\ttraining's gini: 0.332012\tvalid_1's auc: 0.626487\tvalid_1's gini: 0.252974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157]\ttraining's auc: 0.666153\ttraining's gini: 0.332305\tvalid_1's auc: 0.626533\tvalid_1's gini: 0.253066\n",
      "[158]\ttraining's auc: 0.666263\ttraining's gini: 0.332526\tvalid_1's auc: 0.62654\tvalid_1's gini: 0.253079\n",
      "[159]\ttraining's auc: 0.666413\ttraining's gini: 0.332827\tvalid_1's auc: 0.626606\tvalid_1's gini: 0.253212\n",
      "[160]\ttraining's auc: 0.666564\ttraining's gini: 0.333128\tvalid_1's auc: 0.626538\tvalid_1's gini: 0.253076\n",
      "[161]\ttraining's auc: 0.666724\ttraining's gini: 0.333448\tvalid_1's auc: 0.626535\tvalid_1's gini: 0.25307\n",
      "[162]\ttraining's auc: 0.666874\ttraining's gini: 0.333749\tvalid_1's auc: 0.62652\tvalid_1's gini: 0.253039\n",
      "[163]\ttraining's auc: 0.66709\ttraining's gini: 0.33418\tvalid_1's auc: 0.626561\tvalid_1's gini: 0.253123\n",
      "[164]\ttraining's auc: 0.667255\ttraining's gini: 0.334509\tvalid_1's auc: 0.6266\tvalid_1's gini: 0.2532\n",
      "[165]\ttraining's auc: 0.667366\ttraining's gini: 0.334733\tvalid_1's auc: 0.62659\tvalid_1's gini: 0.253179\n",
      "[166]\ttraining's auc: 0.667529\ttraining's gini: 0.335058\tvalid_1's auc: 0.626573\tvalid_1's gini: 0.253146\n",
      "[167]\ttraining's auc: 0.66768\ttraining's gini: 0.33536\tvalid_1's auc: 0.626625\tvalid_1's gini: 0.25325\n",
      "[168]\ttraining's auc: 0.667841\ttraining's gini: 0.335682\tvalid_1's auc: 0.626666\tvalid_1's gini: 0.253332\n",
      "[169]\ttraining's auc: 0.667974\ttraining's gini: 0.335948\tvalid_1's auc: 0.626621\tvalid_1's gini: 0.253242\n",
      "[170]\ttraining's auc: 0.668092\ttraining's gini: 0.336183\tvalid_1's auc: 0.62663\tvalid_1's gini: 0.25326\n",
      "[171]\ttraining's auc: 0.668226\ttraining's gini: 0.336452\tvalid_1's auc: 0.626621\tvalid_1's gini: 0.253241\n",
      "[172]\ttraining's auc: 0.668379\ttraining's gini: 0.336759\tvalid_1's auc: 0.62662\tvalid_1's gini: 0.253239\n",
      "[173]\ttraining's auc: 0.668534\ttraining's gini: 0.337067\tvalid_1's auc: 0.626667\tvalid_1's gini: 0.253333\n",
      "[174]\ttraining's auc: 0.668679\ttraining's gini: 0.337359\tvalid_1's auc: 0.626726\tvalid_1's gini: 0.253453\n",
      "[175]\ttraining's auc: 0.668852\ttraining's gini: 0.337705\tvalid_1's auc: 0.626751\tvalid_1's gini: 0.253502\n",
      "[176]\ttraining's auc: 0.669009\ttraining's gini: 0.338017\tvalid_1's auc: 0.626775\tvalid_1's gini: 0.253551\n",
      "[177]\ttraining's auc: 0.669206\ttraining's gini: 0.338412\tvalid_1's auc: 0.626877\tvalid_1's gini: 0.253754\n",
      "[178]\ttraining's auc: 0.669416\ttraining's gini: 0.338831\tvalid_1's auc: 0.627024\tvalid_1's gini: 0.254048\n",
      "[179]\ttraining's auc: 0.669548\ttraining's gini: 0.339097\tvalid_1's auc: 0.626996\tvalid_1's gini: 0.253991\n",
      "[180]\ttraining's auc: 0.669661\ttraining's gini: 0.339323\tvalid_1's auc: 0.626993\tvalid_1's gini: 0.253986\n",
      "[181]\ttraining's auc: 0.669781\ttraining's gini: 0.339563\tvalid_1's auc: 0.627024\tvalid_1's gini: 0.254048\n",
      "[182]\ttraining's auc: 0.66993\ttraining's gini: 0.339861\tvalid_1's auc: 0.627006\tvalid_1's gini: 0.254012\n",
      "[183]\ttraining's auc: 0.670118\ttraining's gini: 0.340235\tvalid_1's auc: 0.627026\tvalid_1's gini: 0.254052\n",
      "[184]\ttraining's auc: 0.670282\ttraining's gini: 0.340564\tvalid_1's auc: 0.627086\tvalid_1's gini: 0.254172\n",
      "[185]\ttraining's auc: 0.67038\ttraining's gini: 0.340759\tvalid_1's auc: 0.627102\tvalid_1's gini: 0.254203\n",
      "[186]\ttraining's auc: 0.6705\ttraining's gini: 0.340999\tvalid_1's auc: 0.627132\tvalid_1's gini: 0.254264\n",
      "[187]\ttraining's auc: 0.670605\ttraining's gini: 0.34121\tvalid_1's auc: 0.627114\tvalid_1's gini: 0.254228\n",
      "[188]\ttraining's auc: 0.670754\ttraining's gini: 0.341509\tvalid_1's auc: 0.627117\tvalid_1's gini: 0.254235\n",
      "[189]\ttraining's auc: 0.670927\ttraining's gini: 0.341855\tvalid_1's auc: 0.627161\tvalid_1's gini: 0.254322\n",
      "[190]\ttraining's auc: 0.671046\ttraining's gini: 0.342092\tvalid_1's auc: 0.627209\tvalid_1's gini: 0.254418\n",
      "[191]\ttraining's auc: 0.671224\ttraining's gini: 0.342449\tvalid_1's auc: 0.62728\tvalid_1's gini: 0.254559\n",
      "[192]\ttraining's auc: 0.671353\ttraining's gini: 0.342707\tvalid_1's auc: 0.627259\tvalid_1's gini: 0.254518\n",
      "[193]\ttraining's auc: 0.671494\ttraining's gini: 0.342988\tvalid_1's auc: 0.627243\tvalid_1's gini: 0.254485\n",
      "[194]\ttraining's auc: 0.671634\ttraining's gini: 0.343268\tvalid_1's auc: 0.627258\tvalid_1's gini: 0.254516\n",
      "[195]\ttraining's auc: 0.671793\ttraining's gini: 0.343585\tvalid_1's auc: 0.627262\tvalid_1's gini: 0.254523\n",
      "[196]\ttraining's auc: 0.671936\ttraining's gini: 0.343873\tvalid_1's auc: 0.627315\tvalid_1's gini: 0.25463\n",
      "[197]\ttraining's auc: 0.672087\ttraining's gini: 0.344174\tvalid_1's auc: 0.627253\tvalid_1's gini: 0.254506\n",
      "[198]\ttraining's auc: 0.672293\ttraining's gini: 0.344586\tvalid_1's auc: 0.627375\tvalid_1's gini: 0.254751\n",
      "[199]\ttraining's auc: 0.672456\ttraining's gini: 0.344913\tvalid_1's auc: 0.627406\tvalid_1's gini: 0.254811\n",
      "[200]\ttraining's auc: 0.672588\ttraining's gini: 0.345177\tvalid_1's auc: 0.627442\tvalid_1's gini: 0.254884\n",
      "[201]\ttraining's auc: 0.672771\ttraining's gini: 0.345542\tvalid_1's auc: 0.627452\tvalid_1's gini: 0.254904\n",
      "[202]\ttraining's auc: 0.672949\ttraining's gini: 0.345897\tvalid_1's auc: 0.627435\tvalid_1's gini: 0.25487\n",
      "[203]\ttraining's auc: 0.673089\ttraining's gini: 0.346179\tvalid_1's auc: 0.627372\tvalid_1's gini: 0.254745\n",
      "[204]\ttraining's auc: 0.673195\ttraining's gini: 0.346391\tvalid_1's auc: 0.627366\tvalid_1's gini: 0.254732\n",
      "[205]\ttraining's auc: 0.673337\ttraining's gini: 0.346674\tvalid_1's auc: 0.627377\tvalid_1's gini: 0.254753\n",
      "[206]\ttraining's auc: 0.673467\ttraining's gini: 0.346934\tvalid_1's auc: 0.627377\tvalid_1's gini: 0.254753\n",
      "[207]\ttraining's auc: 0.673619\ttraining's gini: 0.347237\tvalid_1's auc: 0.627354\tvalid_1's gini: 0.254707\n",
      "[208]\ttraining's auc: 0.673795\ttraining's gini: 0.347589\tvalid_1's auc: 0.62739\tvalid_1's gini: 0.254779\n",
      "[209]\ttraining's auc: 0.673933\ttraining's gini: 0.347867\tvalid_1's auc: 0.627399\tvalid_1's gini: 0.254799\n",
      "[210]\ttraining's auc: 0.674076\ttraining's gini: 0.348152\tvalid_1's auc: 0.627447\tvalid_1's gini: 0.254894\n",
      "[211]\ttraining's auc: 0.674204\ttraining's gini: 0.348409\tvalid_1's auc: 0.627528\tvalid_1's gini: 0.255057\n",
      "[212]\ttraining's auc: 0.674314\ttraining's gini: 0.348627\tvalid_1's auc: 0.627505\tvalid_1's gini: 0.255011\n",
      "[213]\ttraining's auc: 0.674456\ttraining's gini: 0.348913\tvalid_1's auc: 0.627504\tvalid_1's gini: 0.255007\n",
      "[214]\ttraining's auc: 0.674576\ttraining's gini: 0.349152\tvalid_1's auc: 0.627479\tvalid_1's gini: 0.254957\n",
      "[215]\ttraining's auc: 0.674719\ttraining's gini: 0.349438\tvalid_1's auc: 0.627466\tvalid_1's gini: 0.254931\n",
      "[216]\ttraining's auc: 0.674893\ttraining's gini: 0.349786\tvalid_1's auc: 0.627517\tvalid_1's gini: 0.255034\n",
      "[217]\ttraining's auc: 0.675041\ttraining's gini: 0.350082\tvalid_1's auc: 0.627505\tvalid_1's gini: 0.255009\n",
      "[218]\ttraining's auc: 0.675193\ttraining's gini: 0.350386\tvalid_1's auc: 0.627541\tvalid_1's gini: 0.255083\n",
      "[219]\ttraining's auc: 0.675329\ttraining's gini: 0.350657\tvalid_1's auc: 0.627553\tvalid_1's gini: 0.255106\n",
      "[220]\ttraining's auc: 0.675486\ttraining's gini: 0.350972\tvalid_1's auc: 0.6276\tvalid_1's gini: 0.2552\n",
      "[221]\ttraining's auc: 0.675609\ttraining's gini: 0.351218\tvalid_1's auc: 0.62762\tvalid_1's gini: 0.255241\n",
      "[222]\ttraining's auc: 0.675759\ttraining's gini: 0.351518\tvalid_1's auc: 0.627645\tvalid_1's gini: 0.255291\n",
      "[223]\ttraining's auc: 0.675889\ttraining's gini: 0.351778\tvalid_1's auc: 0.627639\tvalid_1's gini: 0.255278\n",
      "[224]\ttraining's auc: 0.676033\ttraining's gini: 0.352067\tvalid_1's auc: 0.627636\tvalid_1's gini: 0.255273\n",
      "[225]\ttraining's auc: 0.67618\ttraining's gini: 0.352359\tvalid_1's auc: 0.627723\tvalid_1's gini: 0.255446\n",
      "[226]\ttraining's auc: 0.676358\ttraining's gini: 0.352715\tvalid_1's auc: 0.627815\tvalid_1's gini: 0.25563\n",
      "[227]\ttraining's auc: 0.676461\ttraining's gini: 0.352923\tvalid_1's auc: 0.627824\tvalid_1's gini: 0.255649\n",
      "[228]\ttraining's auc: 0.676621\ttraining's gini: 0.353241\tvalid_1's auc: 0.627923\tvalid_1's gini: 0.255847\n",
      "[229]\ttraining's auc: 0.676771\ttraining's gini: 0.353542\tvalid_1's auc: 0.627923\tvalid_1's gini: 0.255847\n",
      "[230]\ttraining's auc: 0.676891\ttraining's gini: 0.353783\tvalid_1's auc: 0.6279\tvalid_1's gini: 0.255799\n",
      "[231]\ttraining's auc: 0.67701\ttraining's gini: 0.35402\tvalid_1's auc: 0.627876\tvalid_1's gini: 0.255752\n",
      "[232]\ttraining's auc: 0.677186\ttraining's gini: 0.354373\tvalid_1's auc: 0.627947\tvalid_1's gini: 0.255895\n",
      "[233]\ttraining's auc: 0.677331\ttraining's gini: 0.354663\tvalid_1's auc: 0.627977\tvalid_1's gini: 0.255954\n",
      "[234]\ttraining's auc: 0.677467\ttraining's gini: 0.354933\tvalid_1's auc: 0.628103\tvalid_1's gini: 0.256206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[235]\ttraining's auc: 0.677585\ttraining's gini: 0.35517\tvalid_1's auc: 0.628173\tvalid_1's gini: 0.256347\n",
      "[236]\ttraining's auc: 0.677699\ttraining's gini: 0.355399\tvalid_1's auc: 0.628192\tvalid_1's gini: 0.256383\n",
      "[237]\ttraining's auc: 0.677854\ttraining's gini: 0.355708\tvalid_1's auc: 0.62822\tvalid_1's gini: 0.25644\n",
      "[238]\ttraining's auc: 0.677959\ttraining's gini: 0.355917\tvalid_1's auc: 0.628193\tvalid_1's gini: 0.256385\n",
      "[239]\ttraining's auc: 0.678092\ttraining's gini: 0.356183\tvalid_1's auc: 0.628192\tvalid_1's gini: 0.256385\n",
      "[240]\ttraining's auc: 0.678196\ttraining's gini: 0.356392\tvalid_1's auc: 0.62819\tvalid_1's gini: 0.25638\n",
      "[241]\ttraining's auc: 0.678336\ttraining's gini: 0.356672\tvalid_1's auc: 0.628203\tvalid_1's gini: 0.256406\n",
      "[242]\ttraining's auc: 0.678498\ttraining's gini: 0.356995\tvalid_1's auc: 0.628302\tvalid_1's gini: 0.256604\n",
      "[243]\ttraining's auc: 0.678642\ttraining's gini: 0.357284\tvalid_1's auc: 0.628316\tvalid_1's gini: 0.256633\n",
      "[244]\ttraining's auc: 0.678783\ttraining's gini: 0.357566\tvalid_1's auc: 0.628402\tvalid_1's gini: 0.256804\n",
      "[245]\ttraining's auc: 0.678885\ttraining's gini: 0.357771\tvalid_1's auc: 0.628417\tvalid_1's gini: 0.256833\n",
      "[246]\ttraining's auc: 0.679026\ttraining's gini: 0.358052\tvalid_1's auc: 0.62842\tvalid_1's gini: 0.25684\n",
      "[247]\ttraining's auc: 0.679152\ttraining's gini: 0.358305\tvalid_1's auc: 0.6284\tvalid_1's gini: 0.2568\n",
      "[248]\ttraining's auc: 0.679256\ttraining's gini: 0.358512\tvalid_1's auc: 0.628415\tvalid_1's gini: 0.256829\n",
      "[249]\ttraining's auc: 0.679368\ttraining's gini: 0.358735\tvalid_1's auc: 0.628439\tvalid_1's gini: 0.256877\n",
      "[250]\ttraining's auc: 0.679496\ttraining's gini: 0.358992\tvalid_1's auc: 0.628387\tvalid_1's gini: 0.256774\n",
      "[251]\ttraining's auc: 0.679628\ttraining's gini: 0.359256\tvalid_1's auc: 0.628397\tvalid_1's gini: 0.256794\n",
      "[252]\ttraining's auc: 0.679782\ttraining's gini: 0.359563\tvalid_1's auc: 0.628471\tvalid_1's gini: 0.256943\n",
      "[253]\ttraining's auc: 0.679917\ttraining's gini: 0.359833\tvalid_1's auc: 0.628464\tvalid_1's gini: 0.256928\n",
      "[254]\ttraining's auc: 0.680067\ttraining's gini: 0.360135\tvalid_1's auc: 0.628446\tvalid_1's gini: 0.256892\n",
      "[255]\ttraining's auc: 0.680195\ttraining's gini: 0.360389\tvalid_1's auc: 0.628531\tvalid_1's gini: 0.257061\n",
      "[256]\ttraining's auc: 0.68031\ttraining's gini: 0.360621\tvalid_1's auc: 0.628516\tvalid_1's gini: 0.257032\n",
      "[257]\ttraining's auc: 0.680412\ttraining's gini: 0.360823\tvalid_1's auc: 0.628492\tvalid_1's gini: 0.256985\n",
      "[258]\ttraining's auc: 0.68057\ttraining's gini: 0.361141\tvalid_1's auc: 0.62856\tvalid_1's gini: 0.25712\n",
      "[259]\ttraining's auc: 0.680721\ttraining's gini: 0.361442\tvalid_1's auc: 0.628579\tvalid_1's gini: 0.257158\n",
      "[260]\ttraining's auc: 0.680834\ttraining's gini: 0.361668\tvalid_1's auc: 0.628586\tvalid_1's gini: 0.257171\n",
      "[261]\ttraining's auc: 0.681019\ttraining's gini: 0.362038\tvalid_1's auc: 0.628619\tvalid_1's gini: 0.257237\n",
      "[262]\ttraining's auc: 0.681151\ttraining's gini: 0.362302\tvalid_1's auc: 0.628673\tvalid_1's gini: 0.257346\n",
      "[263]\ttraining's auc: 0.681266\ttraining's gini: 0.362533\tvalid_1's auc: 0.628673\tvalid_1's gini: 0.257347\n",
      "[264]\ttraining's auc: 0.681429\ttraining's gini: 0.362858\tvalid_1's auc: 0.628712\tvalid_1's gini: 0.257424\n",
      "[265]\ttraining's auc: 0.681588\ttraining's gini: 0.363177\tvalid_1's auc: 0.628751\tvalid_1's gini: 0.257501\n",
      "[266]\ttraining's auc: 0.681696\ttraining's gini: 0.363392\tvalid_1's auc: 0.628708\tvalid_1's gini: 0.257416\n",
      "[267]\ttraining's auc: 0.681886\ttraining's gini: 0.363772\tvalid_1's auc: 0.628784\tvalid_1's gini: 0.257568\n",
      "[268]\ttraining's auc: 0.682027\ttraining's gini: 0.364055\tvalid_1's auc: 0.628766\tvalid_1's gini: 0.257531\n",
      "[269]\ttraining's auc: 0.682161\ttraining's gini: 0.364321\tvalid_1's auc: 0.628771\tvalid_1's gini: 0.257542\n",
      "[270]\ttraining's auc: 0.682297\ttraining's gini: 0.364595\tvalid_1's auc: 0.628769\tvalid_1's gini: 0.257538\n",
      "[271]\ttraining's auc: 0.682456\ttraining's gini: 0.364911\tvalid_1's auc: 0.6288\tvalid_1's gini: 0.257599\n",
      "[272]\ttraining's auc: 0.682561\ttraining's gini: 0.365121\tvalid_1's auc: 0.628833\tvalid_1's gini: 0.257666\n",
      "[273]\ttraining's auc: 0.682703\ttraining's gini: 0.365405\tvalid_1's auc: 0.628834\tvalid_1's gini: 0.257667\n",
      "[274]\ttraining's auc: 0.682832\ttraining's gini: 0.365663\tvalid_1's auc: 0.628853\tvalid_1's gini: 0.257706\n",
      "[275]\ttraining's auc: 0.682955\ttraining's gini: 0.365909\tvalid_1's auc: 0.628877\tvalid_1's gini: 0.257755\n",
      "[276]\ttraining's auc: 0.683084\ttraining's gini: 0.366168\tvalid_1's auc: 0.628926\tvalid_1's gini: 0.257852\n",
      "[277]\ttraining's auc: 0.683215\ttraining's gini: 0.366429\tvalid_1's auc: 0.628974\tvalid_1's gini: 0.257948\n",
      "[278]\ttraining's auc: 0.683343\ttraining's gini: 0.366686\tvalid_1's auc: 0.629013\tvalid_1's gini: 0.258026\n",
      "[279]\ttraining's auc: 0.683451\ttraining's gini: 0.366902\tvalid_1's auc: 0.629019\tvalid_1's gini: 0.258037\n",
      "[280]\ttraining's auc: 0.683532\ttraining's gini: 0.367065\tvalid_1's auc: 0.628988\tvalid_1's gini: 0.257976\n",
      "[281]\ttraining's auc: 0.683654\ttraining's gini: 0.367307\tvalid_1's auc: 0.628988\tvalid_1's gini: 0.257976\n",
      "[282]\ttraining's auc: 0.683748\ttraining's gini: 0.367496\tvalid_1's auc: 0.628951\tvalid_1's gini: 0.257902\n",
      "[283]\ttraining's auc: 0.683901\ttraining's gini: 0.367803\tvalid_1's auc: 0.628943\tvalid_1's gini: 0.257885\n",
      "[284]\ttraining's auc: 0.684022\ttraining's gini: 0.368043\tvalid_1's auc: 0.62901\tvalid_1's gini: 0.25802\n",
      "[285]\ttraining's auc: 0.684165\ttraining's gini: 0.36833\tvalid_1's auc: 0.628995\tvalid_1's gini: 0.257991\n",
      "[286]\ttraining's auc: 0.6843\ttraining's gini: 0.368601\tvalid_1's auc: 0.629008\tvalid_1's gini: 0.258016\n",
      "[287]\ttraining's auc: 0.684439\ttraining's gini: 0.368878\tvalid_1's auc: 0.629021\tvalid_1's gini: 0.258042\n",
      "[288]\ttraining's auc: 0.684539\ttraining's gini: 0.369077\tvalid_1's auc: 0.628998\tvalid_1's gini: 0.257997\n",
      "[289]\ttraining's auc: 0.684661\ttraining's gini: 0.369322\tvalid_1's auc: 0.629005\tvalid_1's gini: 0.258011\n",
      "[290]\ttraining's auc: 0.68476\ttraining's gini: 0.369521\tvalid_1's auc: 0.629014\tvalid_1's gini: 0.258029\n",
      "[291]\ttraining's auc: 0.684883\ttraining's gini: 0.369765\tvalid_1's auc: 0.629011\tvalid_1's gini: 0.258022\n",
      "[292]\ttraining's auc: 0.685023\ttraining's gini: 0.370047\tvalid_1's auc: 0.629053\tvalid_1's gini: 0.258106\n",
      "[293]\ttraining's auc: 0.685171\ttraining's gini: 0.370343\tvalid_1's auc: 0.62906\tvalid_1's gini: 0.25812\n",
      "[294]\ttraining's auc: 0.685289\ttraining's gini: 0.370578\tvalid_1's auc: 0.629053\tvalid_1's gini: 0.258105\n",
      "[295]\ttraining's auc: 0.685404\ttraining's gini: 0.370809\tvalid_1's auc: 0.629069\tvalid_1's gini: 0.258138\n",
      "[296]\ttraining's auc: 0.685581\ttraining's gini: 0.371161\tvalid_1's auc: 0.62912\tvalid_1's gini: 0.25824\n",
      "[297]\ttraining's auc: 0.685715\ttraining's gini: 0.37143\tvalid_1's auc: 0.629165\tvalid_1's gini: 0.258331\n",
      "[298]\ttraining's auc: 0.68584\ttraining's gini: 0.37168\tvalid_1's auc: 0.629184\tvalid_1's gini: 0.258367\n",
      "[299]\ttraining's auc: 0.685952\ttraining's gini: 0.371904\tvalid_1's auc: 0.629205\tvalid_1's gini: 0.25841\n",
      "[300]\ttraining's auc: 0.686097\ttraining's gini: 0.372193\tvalid_1's auc: 0.629233\tvalid_1's gini: 0.258467\n",
      "[301]\ttraining's auc: 0.686286\ttraining's gini: 0.372572\tvalid_1's auc: 0.629288\tvalid_1's gini: 0.258576\n",
      "[302]\ttraining's auc: 0.686452\ttraining's gini: 0.372903\tvalid_1's auc: 0.629276\tvalid_1's gini: 0.258552\n",
      "[303]\ttraining's auc: 0.686574\ttraining's gini: 0.373147\tvalid_1's auc: 0.629309\tvalid_1's gini: 0.258618\n",
      "[304]\ttraining's auc: 0.686702\ttraining's gini: 0.373403\tvalid_1's auc: 0.629298\tvalid_1's gini: 0.258596\n",
      "[305]\ttraining's auc: 0.686788\ttraining's gini: 0.373575\tvalid_1's auc: 0.629317\tvalid_1's gini: 0.258633\n",
      "[306]\ttraining's auc: 0.686905\ttraining's gini: 0.373809\tvalid_1's auc: 0.629355\tvalid_1's gini: 0.25871\n",
      "[307]\ttraining's auc: 0.687032\ttraining's gini: 0.374063\tvalid_1's auc: 0.629372\tvalid_1's gini: 0.258743\n",
      "[308]\ttraining's auc: 0.687147\ttraining's gini: 0.374295\tvalid_1's auc: 0.629319\tvalid_1's gini: 0.258638\n",
      "[309]\ttraining's auc: 0.687293\ttraining's gini: 0.374586\tvalid_1's auc: 0.629391\tvalid_1's gini: 0.258782\n",
      "[310]\ttraining's auc: 0.687429\ttraining's gini: 0.374859\tvalid_1's auc: 0.629408\tvalid_1's gini: 0.258817\n",
      "[311]\ttraining's auc: 0.687598\ttraining's gini: 0.375197\tvalid_1's auc: 0.629405\tvalid_1's gini: 0.25881\n",
      "[312]\ttraining's auc: 0.687747\ttraining's gini: 0.375495\tvalid_1's auc: 0.629423\tvalid_1's gini: 0.258846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[313]\ttraining's auc: 0.687848\ttraining's gini: 0.375697\tvalid_1's auc: 0.629455\tvalid_1's gini: 0.25891\n",
      "[314]\ttraining's auc: 0.68797\ttraining's gini: 0.375941\tvalid_1's auc: 0.62947\tvalid_1's gini: 0.258939\n",
      "[315]\ttraining's auc: 0.688071\ttraining's gini: 0.376141\tvalid_1's auc: 0.629417\tvalid_1's gini: 0.258834\n",
      "[316]\ttraining's auc: 0.688236\ttraining's gini: 0.376472\tvalid_1's auc: 0.629415\tvalid_1's gini: 0.25883\n",
      "[317]\ttraining's auc: 0.688354\ttraining's gini: 0.376708\tvalid_1's auc: 0.629423\tvalid_1's gini: 0.258845\n",
      "[318]\ttraining's auc: 0.688466\ttraining's gini: 0.376933\tvalid_1's auc: 0.629424\tvalid_1's gini: 0.258849\n",
      "[319]\ttraining's auc: 0.688601\ttraining's gini: 0.377201\tvalid_1's auc: 0.629413\tvalid_1's gini: 0.258827\n",
      "[320]\ttraining's auc: 0.688762\ttraining's gini: 0.377523\tvalid_1's auc: 0.629403\tvalid_1's gini: 0.258807\n",
      "[321]\ttraining's auc: 0.688867\ttraining's gini: 0.377734\tvalid_1's auc: 0.629407\tvalid_1's gini: 0.258814\n",
      "[322]\ttraining's auc: 0.688983\ttraining's gini: 0.377966\tvalid_1's auc: 0.629427\tvalid_1's gini: 0.258854\n",
      "[323]\ttraining's auc: 0.689123\ttraining's gini: 0.378247\tvalid_1's auc: 0.629453\tvalid_1's gini: 0.258907\n",
      "[324]\ttraining's auc: 0.689225\ttraining's gini: 0.378449\tvalid_1's auc: 0.629462\tvalid_1's gini: 0.258924\n",
      "[325]\ttraining's auc: 0.689348\ttraining's gini: 0.378696\tvalid_1's auc: 0.629462\tvalid_1's gini: 0.258924\n",
      "[326]\ttraining's auc: 0.689545\ttraining's gini: 0.379091\tvalid_1's auc: 0.629475\tvalid_1's gini: 0.258951\n",
      "[327]\ttraining's auc: 0.689643\ttraining's gini: 0.379286\tvalid_1's auc: 0.629439\tvalid_1's gini: 0.258878\n",
      "[328]\ttraining's auc: 0.689766\ttraining's gini: 0.379531\tvalid_1's auc: 0.629463\tvalid_1's gini: 0.258925\n",
      "[329]\ttraining's auc: 0.689926\ttraining's gini: 0.379852\tvalid_1's auc: 0.629494\tvalid_1's gini: 0.258987\n",
      "[330]\ttraining's auc: 0.690034\ttraining's gini: 0.380067\tvalid_1's auc: 0.629488\tvalid_1's gini: 0.258976\n",
      "[331]\ttraining's auc: 0.690155\ttraining's gini: 0.380309\tvalid_1's auc: 0.629496\tvalid_1's gini: 0.258992\n",
      "[332]\ttraining's auc: 0.690245\ttraining's gini: 0.38049\tvalid_1's auc: 0.629503\tvalid_1's gini: 0.259007\n",
      "[333]\ttraining's auc: 0.690359\ttraining's gini: 0.380719\tvalid_1's auc: 0.629476\tvalid_1's gini: 0.258952\n",
      "[334]\ttraining's auc: 0.690482\ttraining's gini: 0.380963\tvalid_1's auc: 0.629495\tvalid_1's gini: 0.25899\n",
      "[335]\ttraining's auc: 0.690592\ttraining's gini: 0.381184\tvalid_1's auc: 0.629547\tvalid_1's gini: 0.259093\n",
      "[336]\ttraining's auc: 0.690741\ttraining's gini: 0.381483\tvalid_1's auc: 0.629535\tvalid_1's gini: 0.259071\n",
      "[337]\ttraining's auc: 0.690889\ttraining's gini: 0.381778\tvalid_1's auc: 0.629532\tvalid_1's gini: 0.259064\n",
      "[338]\ttraining's auc: 0.691074\ttraining's gini: 0.382148\tvalid_1's auc: 0.629518\tvalid_1's gini: 0.259036\n",
      "[339]\ttraining's auc: 0.691215\ttraining's gini: 0.382429\tvalid_1's auc: 0.629514\tvalid_1's gini: 0.259028\n",
      "[340]\ttraining's auc: 0.691328\ttraining's gini: 0.382656\tvalid_1's auc: 0.629522\tvalid_1's gini: 0.259045\n",
      "[341]\ttraining's auc: 0.691412\ttraining's gini: 0.382824\tvalid_1's auc: 0.629513\tvalid_1's gini: 0.259027\n",
      "[342]\ttraining's auc: 0.691556\ttraining's gini: 0.383113\tvalid_1's auc: 0.629493\tvalid_1's gini: 0.258985\n",
      "[343]\ttraining's auc: 0.691727\ttraining's gini: 0.383453\tvalid_1's auc: 0.629527\tvalid_1's gini: 0.259054\n",
      "[344]\ttraining's auc: 0.691837\ttraining's gini: 0.383673\tvalid_1's auc: 0.629547\tvalid_1's gini: 0.259094\n",
      "[345]\ttraining's auc: 0.691973\ttraining's gini: 0.383946\tvalid_1's auc: 0.629552\tvalid_1's gini: 0.259103\n",
      "[346]\ttraining's auc: 0.692067\ttraining's gini: 0.384135\tvalid_1's auc: 0.629561\tvalid_1's gini: 0.259122\n",
      "[347]\ttraining's auc: 0.692194\ttraining's gini: 0.384388\tvalid_1's auc: 0.629614\tvalid_1's gini: 0.259229\n",
      "[348]\ttraining's auc: 0.692292\ttraining's gini: 0.384585\tvalid_1's auc: 0.629593\tvalid_1's gini: 0.259186\n",
      "[349]\ttraining's auc: 0.692434\ttraining's gini: 0.384868\tvalid_1's auc: 0.629592\tvalid_1's gini: 0.259183\n",
      "[350]\ttraining's auc: 0.692575\ttraining's gini: 0.38515\tvalid_1's auc: 0.629626\tvalid_1's gini: 0.259252\n",
      "[351]\ttraining's auc: 0.692724\ttraining's gini: 0.385447\tvalid_1's auc: 0.629646\tvalid_1's gini: 0.259292\n",
      "[352]\ttraining's auc: 0.692826\ttraining's gini: 0.385653\tvalid_1's auc: 0.629652\tvalid_1's gini: 0.259304\n",
      "[353]\ttraining's auc: 0.692955\ttraining's gini: 0.38591\tvalid_1's auc: 0.629635\tvalid_1's gini: 0.259271\n",
      "[354]\ttraining's auc: 0.693094\ttraining's gini: 0.386187\tvalid_1's auc: 0.629652\tvalid_1's gini: 0.259305\n",
      "[355]\ttraining's auc: 0.693171\ttraining's gini: 0.386341\tvalid_1's auc: 0.629649\tvalid_1's gini: 0.259299\n",
      "[356]\ttraining's auc: 0.693349\ttraining's gini: 0.386697\tvalid_1's auc: 0.629635\tvalid_1's gini: 0.25927\n",
      "[357]\ttraining's auc: 0.693458\ttraining's gini: 0.386916\tvalid_1's auc: 0.62965\tvalid_1's gini: 0.259301\n",
      "[358]\ttraining's auc: 0.693607\ttraining's gini: 0.387213\tvalid_1's auc: 0.629638\tvalid_1's gini: 0.259276\n",
      "[359]\ttraining's auc: 0.69376\ttraining's gini: 0.38752\tvalid_1's auc: 0.629624\tvalid_1's gini: 0.259248\n",
      "[360]\ttraining's auc: 0.693857\ttraining's gini: 0.387713\tvalid_1's auc: 0.629631\tvalid_1's gini: 0.259261\n",
      "[361]\ttraining's auc: 0.693967\ttraining's gini: 0.387935\tvalid_1's auc: 0.629686\tvalid_1's gini: 0.259371\n",
      "[362]\ttraining's auc: 0.694049\ttraining's gini: 0.388099\tvalid_1's auc: 0.629671\tvalid_1's gini: 0.259341\n",
      "[363]\ttraining's auc: 0.694186\ttraining's gini: 0.388373\tvalid_1's auc: 0.62974\tvalid_1's gini: 0.25948\n",
      "[364]\ttraining's auc: 0.694318\ttraining's gini: 0.388637\tvalid_1's auc: 0.629744\tvalid_1's gini: 0.259487\n",
      "[365]\ttraining's auc: 0.694441\ttraining's gini: 0.388881\tvalid_1's auc: 0.629716\tvalid_1's gini: 0.259432\n",
      "[366]\ttraining's auc: 0.694619\ttraining's gini: 0.389238\tvalid_1's auc: 0.62974\tvalid_1's gini: 0.259481\n",
      "[367]\ttraining's auc: 0.694713\ttraining's gini: 0.389426\tvalid_1's auc: 0.629734\tvalid_1's gini: 0.259469\n",
      "[368]\ttraining's auc: 0.694851\ttraining's gini: 0.389701\tvalid_1's auc: 0.629752\tvalid_1's gini: 0.259505\n",
      "[369]\ttraining's auc: 0.694975\ttraining's gini: 0.389949\tvalid_1's auc: 0.629788\tvalid_1's gini: 0.259576\n",
      "[370]\ttraining's auc: 0.695099\ttraining's gini: 0.390198\tvalid_1's auc: 0.629788\tvalid_1's gini: 0.259577\n",
      "[371]\ttraining's auc: 0.695243\ttraining's gini: 0.390486\tvalid_1's auc: 0.629825\tvalid_1's gini: 0.25965\n",
      "[372]\ttraining's auc: 0.69533\ttraining's gini: 0.39066\tvalid_1's auc: 0.629818\tvalid_1's gini: 0.259636\n",
      "[373]\ttraining's auc: 0.695438\ttraining's gini: 0.390875\tvalid_1's auc: 0.629876\tvalid_1's gini: 0.259752\n",
      "[374]\ttraining's auc: 0.695526\ttraining's gini: 0.391052\tvalid_1's auc: 0.629855\tvalid_1's gini: 0.259711\n",
      "[375]\ttraining's auc: 0.695631\ttraining's gini: 0.391262\tvalid_1's auc: 0.629824\tvalid_1's gini: 0.259649\n",
      "[376]\ttraining's auc: 0.695733\ttraining's gini: 0.391466\tvalid_1's auc: 0.629797\tvalid_1's gini: 0.259594\n",
      "[377]\ttraining's auc: 0.695866\ttraining's gini: 0.391731\tvalid_1's auc: 0.62977\tvalid_1's gini: 0.259541\n",
      "[378]\ttraining's auc: 0.696014\ttraining's gini: 0.392027\tvalid_1's auc: 0.629776\tvalid_1's gini: 0.259552\n",
      "[379]\ttraining's auc: 0.696155\ttraining's gini: 0.392311\tvalid_1's auc: 0.629789\tvalid_1's gini: 0.259578\n",
      "[380]\ttraining's auc: 0.696253\ttraining's gini: 0.392506\tvalid_1's auc: 0.629821\tvalid_1's gini: 0.259643\n",
      "[381]\ttraining's auc: 0.69634\ttraining's gini: 0.39268\tvalid_1's auc: 0.629811\tvalid_1's gini: 0.259623\n",
      "[382]\ttraining's auc: 0.696485\ttraining's gini: 0.392969\tvalid_1's auc: 0.629832\tvalid_1's gini: 0.259664\n",
      "[383]\ttraining's auc: 0.696548\ttraining's gini: 0.393096\tvalid_1's auc: 0.629828\tvalid_1's gini: 0.259655\n",
      "[384]\ttraining's auc: 0.696664\ttraining's gini: 0.393328\tvalid_1's auc: 0.629833\tvalid_1's gini: 0.259665\n",
      "[385]\ttraining's auc: 0.696838\ttraining's gini: 0.393675\tvalid_1's auc: 0.629858\tvalid_1's gini: 0.259715\n",
      "[386]\ttraining's auc: 0.696947\ttraining's gini: 0.393895\tvalid_1's auc: 0.629829\tvalid_1's gini: 0.259657\n",
      "[387]\ttraining's auc: 0.697102\ttraining's gini: 0.394205\tvalid_1's auc: 0.62987\tvalid_1's gini: 0.25974\n",
      "[388]\ttraining's auc: 0.697245\ttraining's gini: 0.394489\tvalid_1's auc: 0.629838\tvalid_1's gini: 0.259676\n",
      "[389]\ttraining's auc: 0.697352\ttraining's gini: 0.394703\tvalid_1's auc: 0.629816\tvalid_1's gini: 0.259632\n",
      "[390]\ttraining's auc: 0.697504\ttraining's gini: 0.395008\tvalid_1's auc: 0.629827\tvalid_1's gini: 0.259654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[391]\ttraining's auc: 0.697601\ttraining's gini: 0.395201\tvalid_1's auc: 0.629802\tvalid_1's gini: 0.259603\n",
      "[392]\ttraining's auc: 0.697724\ttraining's gini: 0.395448\tvalid_1's auc: 0.629809\tvalid_1's gini: 0.259619\n",
      "[393]\ttraining's auc: 0.697842\ttraining's gini: 0.395684\tvalid_1's auc: 0.629811\tvalid_1's gini: 0.259623\n",
      "Early stopping, best iteration is:\n",
      "[373]\ttraining's auc: 0.695438\ttraining's gini: 0.390875\tvalid_1's auc: 0.629876\tvalid_1's gini: 0.259752\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'application': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.02,\n",
    "    'is_unbalance': True,\n",
    "#     'max_depth': 5,\n",
    "#     'num_leaves': 25,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.5,\n",
    "#     'reg_alpha': 0.1,\n",
    "    'seed': 0,\n",
    "}\n",
    "bst = lgb.train(params, trn_lgb, 1000, \n",
    "                valid_sets=[trn_lgb, val_lgb], \n",
    "                early_stopping_rounds=20, \n",
    "                feval=gini_lgb,\n",
    "                feature_name=feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bst = lgb.train(params, trn_lgb, 1000, valid_sets=[trn_lgb, val_lgb], early_stopping_rounds=20,\n",
    "#                 init_model=bst, \n",
    "#                 learning_rates=lambda iter: 0.1 * (0.99 ** iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imp_df = pd.DataFrame([bst.feature_importance()], columns=feature_columns, index=['importance']).T.sort_values(by='importance', ascending=False)\n",
    "# imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(bst.predict(X_tst), columns=[output_label_col])\n",
    "res_df[output_id_col] = tst_df[output_id_col]\n",
    "res_df[[output_id_col, output_label_col]].to_csv(submission_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
