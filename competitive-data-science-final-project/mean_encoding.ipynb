{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Encodings\n",
    "\n",
    "  1. KFold scheme\n",
    "  2. Leave-one-out scheme\n",
    "  3. smoothing scheme\n",
    "  4. expanding mean scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv('./data/input/sales_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_df['revenue'] = sales_df['item_price'] * sales_df['item_cnt_day']\n",
    "full_df = sales_df.groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].sum().reset_index()\n",
    "full_df = full_df.rename(columns={'item_cnt_day': 'item_cnt_month'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dbn in range(0, 34):\n",
    "    # 生成所有可能的(shop_id, item_id)\n",
    "    unique_shop_id = full_df[full_df['date_block_num'] == dbn]['shop_id'].copy().drop_duplicates()\n",
    "    unique_item_id = full_df[full_df['date_block_num'] == dbn]['item_id'].copy().drop_duplicates()\n",
    "    m_index = pd.MultiIndex.from_product([unique_shop_id, unique_item_id], names=['shop_id', 'item_id'])\n",
    "    val_df = pd.DataFrame([], index=m_index).reset_index()\n",
    "    val_df['date_block_num'] = dbn\n",
    "    # 去掉已经有的(shop_id, item_id)\n",
    "    origin_df = full_df[full_df['date_block_num'] == dbn][['item_id', 'shop_id', 'item_cnt_month']]\n",
    "    val_df = val_df.merge(origin_df, how='left', on=['item_id', 'shop_id'])\n",
    "    val_df = val_df[val_df.item_cnt_month.isnull()]\n",
    "    # 没有的记录 说明销售为0\n",
    "    val_df['item_cnt_month'] = 0\n",
    "\n",
    "    # 合并到full_df\n",
    "    full_df = pd.concat([full_df, val_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_df['target'] = full_df['item_cnt_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globalmean = full_df['target'].mean()\n",
    "globalmean = 0.3343"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean encoding without regularization\n",
    "\n",
    "使用所有`item_id`的均值作为encoding，不进行任何regularization\n",
    "\n",
    "这种方式会出现过拟合的现象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48303869886213624\n"
     ]
    }
   ],
   "source": [
    "# 方法1\n",
    "item_id_target_mean = full_df.groupby('item_id').target.mean()\n",
    "full_df['item_target_enc'] = full_df['item_id'].map(item_id_target_mean)\n",
    "full_df['item_target_enc'].fillna(globalmean, inplace=True) \n",
    "\n",
    "# Print correlation\n",
    "encoded_feature = full_df['item_target_enc'].values\n",
    "print(np.corrcoef(full_df['target'].values, encoded_feature)[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48303869886213624\n"
     ]
    }
   ],
   "source": [
    "# 方法2\n",
    "full_df['item_target_enc'] = full_df.groupby('item_id')['target'].transform('mean')\n",
    "full_df['item_target_enc'].fillna(globalmean, inplace=True) \n",
    "\n",
    "# Print correlation\n",
    "encoded_feature = full_df['item_target_enc'].values\n",
    "print(np.corrcoef(full_df['target'].values, encoded_feature)[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来使用regularization的方法来减轻过拟合的状况\n",
    "\n",
    "## 1. KFold scheme\n",
    "\n",
    "  对原始数据进行KFold，对于每个fold，使用训练集得到mean_encoding，再赋值到验证集上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4803001456002725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(5, shuffle=True, random_state=20180717)\n",
    "for trn_idx, val_idx in kf.split(full_df['target'].values):\n",
    "    trn_df, val_df = full_df.iloc[trn_idx].copy(), full_df.iloc[val_idx].copy()\n",
    "    item_id_target_mean = trn_df.groupby('item_id')['target'].mean()\n",
    "    val_df['item_target_enc'] = val_df['item_id'].map(item_id_target_mean)\n",
    "    full_df.iloc[val_idx] = val_df\n",
    "full_df.fillna(globalmean, inplace=True)\n",
    "\n",
    "# You will need to compute correlation like that\n",
    "encoded_feature = full_df['item_target_enc'].values\n",
    "corr = np.corrcoef(full_df['target'].values, encoded_feature)[0][1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leave-one-out scheme\n",
    "\n",
    "  对原始数据进行Leave-one-out，也就是除了当前的这一行，使用其他所有行进行mean_encoding，再赋值到当前行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4803848311292604\n"
     ]
    }
   ],
   "source": [
    "target_sum = full_df.groupby('item_id')['target'].transform('sum')\n",
    "target_count = full_df.groupby('item_id')['target'].transform('count')\n",
    "encoded_feature = (target_sum - full_df['target']) / (target_count - 1)\n",
    "encoded_feature.fillna(globalmean, inplace=True)\n",
    "\n",
    "corr = np.corrcoef(full_df['target'].values, encoded_feature)[0][1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Smoothing scheme\n",
    "\n",
    "  使用以下公式进行平滑：$$\\frac{mean(target)*nrows+globalmean*alpha}{nrows+alpha}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4818198797096877\n"
     ]
    }
   ],
   "source": [
    "alpha = 100\n",
    "target_mean = full_df.groupby('item_id')['target'].transform('mean')\n",
    "target_count = full_df.groupby('item_id')['target'].transform('count')\n",
    "encoded_feature = (target_mean * target_count + globalmean * alpha) / (target_count + alpha)\n",
    "encoded_feature.fillna(globalmean, inplace=True)\n",
    "\n",
    "corr = np.corrcoef(full_df['target'].values, encoded_feature)[0][1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Expanding mean scheme\n",
    "\n",
    "  使用到当前的累计求和除以累计计数得到mean_encoding（不包括当前行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4540235160517741\n"
     ]
    }
   ],
   "source": [
    "encoded_feature = (full_df.groupby('item_id')['target'].cumsum() - full_df['target']) / full_df.groupby('item_id')['target'].cumcount()\n",
    "encoded_feature.fillna(globalmean, inplace=True)\n",
    "\n",
    "corr = np.corrcoef(full_df['target'].values, encoded_feature)[0][1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
